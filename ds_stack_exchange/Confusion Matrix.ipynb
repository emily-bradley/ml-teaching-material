{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f84769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 8\n",
      "Precision Score: 0.75\n",
      "Recall Score: 0.6\n",
      "Accuracy Score: 0.625\n",
      "Number of Positive Predictions: 5.0 \n",
      "Number of Negative Predictions: 3.0\n",
      "FN: 2.0\n",
      "TP: 3.0\n",
      "TN: 2.0\n",
      "FP: 1.0\n",
      "Verify our results using Sklearn confusion matrix values\n",
      "FN: 2\n",
      "TP: 3\n",
      "TN: 2\n",
      "FP: 1\n"
     ]
    }
   ],
   "source": [
    "# Calculating FP, FN, TP, and TN using accuracy, precision, and recall\n",
    "# https://datascience.stackexchange.com/questions/61114/confusion-matrix-determine-the-values-of-fp-fn-tp-and-tn\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [0, 1, 0, 0, 1, 1, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 0, 1, 1, 1]\n",
    "\n",
    "num_values = len(y_true)\n",
    "print(\"Number of observations: {}\".format(num_values))\n",
    "\n",
    "# Calculate Precision\n",
    "# The precision is the ratio tp / (tp + fp)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(\"Precision Score: {}\".format(precision))\n",
    "\n",
    "# Calculate Recall\n",
    "# The recall is the ratio tp / (tp + fn)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(\"Recall Score: {}\".format(recall))\n",
    "\n",
    "# Calculate Accuracy\n",
    "# The accuracy is the ratio (TP + TN)/(TP + TN + FP + FN)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy Score: {}\".format(accuracy))\n",
    "\n",
    "# Calculate the number of positive predictions\n",
    "num_pos_preds = accuracy * num_values\n",
    "num_neg_preds = num_values - num_pos_preds\n",
    "print(\"Number of Positive Predictions: {0} \\n\"\n",
    "      \"Number of Negative Predictions: {1}\".format(num_pos_preds, num_neg_preds))\n",
    "\n",
    "# Calculate the False Negatives\n",
    "FN = num_pos_preds * (1 - recall)\n",
    "print(\"FN: {0}\".format(FN))\n",
    "\n",
    "# Calculate the True Positives\n",
    "TP = num_pos_preds - FN\n",
    "print(\"TP: {0}\".format(TP))\n",
    "\n",
    "# Calculate the True Negatives\n",
    "TN = num_pos_preds - TP\n",
    "print(\"TN: {0}\".format(TN))\n",
    "\n",
    "# Calculate the False Positives\n",
    "FP = num_neg_preds - TN\n",
    "print(\"FP: {0}\".format(FP))\n",
    "\n",
    "# Verify the results\n",
    "sk_tn, sk_fp, sk_fn, sk_tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(\"Verify our results using Sklearn confusion matrix values\\n\"\n",
    "      \"FN: {0}\\n\"\n",
    "      \"TP: {1}\\n\"\n",
    "      \"TN: {2}\\n\"\n",
    "      \"FP: {3}\".format(sk_fn, sk_tp, sk_tn, sk_fp,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b02dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
