{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9538306d",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6899a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c7f7ece",
   "metadata": {},
   "source": [
    "## Decision Tree Implementation by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c1724a",
   "metadata": {},
   "source": [
    "Decision tree algorithm: CART - Classification and Regression Trees\n",
    "* Form a binary tree and minimize the error in each leaf of the tree (greed algorithm)\n",
    "* Each node represents a single input variable (x)\n",
    "* Each leaf represents an output variable (y)\n",
    "* Given new input, the tree is traversed by evaluating the specific input starting at the root node of the tree\n",
    "* Recursive binary splitting - that is, you choose a sequence of binary splits (True and False)\n",
    "* Goal: unimix the labels as we proceed down; to provide the purest possible distribution of the labels at each node\n",
    "\n",
    "\n",
    "* When the input into a node only contains a certain type of a label, it is perfectly unmixed (there is no uncertainty about that type of label).\n",
    "* When input is still mixed, we need to ask more questions\n",
    "* To build an effective tree, we must understand which questiosn to ask and when to ask them. To do that, we need to quantify how much a label helps to unmix the lables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578dd92",
   "metadata": {},
   "source": [
    "1. Step 1: Add a root to the node of the tree\n",
    "* All nodes receive a list of rows as output\n",
    "* The root will receive the entire training set\n",
    "* Each node asks a T/F question about one of the features\n",
    "\n",
    "\n",
    "2. We split or partition the data into two subsets based on the T/F questions; We do this by finding the split with the lowest cost. This is the sum squarred error across all training samples that fall within the range (sum y-precision)^2\n",
    "* The finin index function is used to decide how 'pure' a function is (how mixed the training data assigned to each node is)\n",
    "* **Gini impurity** - a metric that can quanitfy the amount of uncertainty in a single node\n",
    "    * G = sum(pk*(1-pk))\n",
    "    * G is the gini index over all classes\n",
    "    * pk is the proportion of training instances with class k\n",
    "    * a node with all classes of the same type gives us G=0\n",
    "    * a node with a 50/50 split gives us G=0.5\n",
    "* **Information Gain ** - a metric that can quantify how much a questiond reduces the gini impurity (uncertainty)\n",
    "    * Gini index calcuation for each nodes is weighted by th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc32512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
